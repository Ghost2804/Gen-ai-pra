{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZadCaSMNDUZ"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUhbKn5dOrZm"
      },
      "source": [
        "#Practical no 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFbBqjYRXhdm"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "c = tf.constant(np.array([[7,2],[3,4]]))\n",
        "print(c)\n",
        "\n",
        "print(c[0])\n",
        "print(c[1,0])\n",
        "\n",
        "print(tf.reshape(c,[1,4]))\n",
        "\n",
        "f = c+ c\n",
        "print(f)\n",
        "\n",
        "g = tf.matmul(c,c)\n",
        "print('a @ a:\\n',g)\n",
        "\n",
        "\n",
        "x = tf.constant(3.0)\n",
        "y = x * 2 + 1  # Immediate evaluation\n",
        "print(\"Eager: x*2 + 1 =\", y)\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-l1ONY1XlA-"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7fySbj4XklY"
      },
      "source": [
        "# Practical no 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vXCBFh09Xhgu"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "data_group1 = np.random.normal(loc = 10 , scale = 10 , size = 20)\n",
        "data_group2 = np.random.normal(loc = 100 , scale = 10 , size = 20)\n",
        "\n",
        "plt.hist(data_group1,label=\"Group1\", color = \"blue\")\n",
        "plt.hist(data_group2,label=\"Group2\", color = \"red\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AWmBn6EXkIO"
      },
      "source": [
        "#practical no 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ozQqhy3Xhjn"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "sentences = [\n",
        "    \"artificial intelligence is cool\",\n",
        "    \"machine learning is fun\",\n",
        "    \"ai learning uses neural netowrks\"\n",
        "]\n",
        "\n",
        "model = TfidfVectorizer()\n",
        "x = model.fit_transform(sentences)\n",
        "\n",
        "print(\"Vocabolary\", model.get_feature_names_out())\n",
        "print(\"Most similar to learining :\", x.toarray()[:,model.vocabulary_[\"is\"]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upyga2cVXjpm"
      },
      "source": [
        "#practical no 5+\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HwNd0MSfXhmm"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf, numpy as np, matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "(x,_),_=tf.keras.datasets.mnist.load_data()\n",
        "x=x/255.; x=np.expand_dims(x,-1)\n",
        "\n",
        "\n",
        "i=tf.keras.Input((28,28,1))\n",
        "o=tf.keras.layers.Conv2D(32,3,padding=\"same\",activation=\"relu\")(i)\n",
        "o=tf.keras.layers.Conv2D(1,3,padding=\"same\")(o)\n",
        "model=tf.keras.Model(i,o)\n",
        "\n",
        "\n",
        "img=tf.random.normal((1,28,28,1))\n",
        "for _ in range(80):\n",
        "  img = img-model(img)*0.1+tf.random.normal(img.shape)*0.02\n",
        "\n",
        "plt.imshow(tf.squeeze(tf.clip_by_value(img,0,1)),cmap=\"gray\"); plt.axis(\"off\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wf5KrEb6vrkR"
      },
      "source": [
        "# Practical no 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TiDmSouyN31u"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install diffusers transformers accelerate torch\n",
        "\n",
        "from diffusers import StableDiffusionPipeline\n",
        "import torch\n",
        "\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\")\n",
        "pipe = pipe.to(\"cpu\")   # Use CPU for generation\n",
        "\n",
        "prompt = \"A beautiful scenic sunset over mountains\"\n",
        "\n",
        "image = pipe(prompt).images[0]\n",
        "\n",
        "image.show()\n",
        "image.save(\"generated_image.png\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVLuxw9YPhGZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQS1SZ36Pz6R"
      },
      "source": [
        "# Practical no 7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQesyTg7QSU-"
      },
      "outputs": [],
      "source": [
        "!pip install transformers peft torch accelerate -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hk6gR95-NRDJ"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "import math\n",
        "\n",
        "\n",
        "name = \"distilgpt2\"\n",
        "tok = AutoTokenizer.from_pretrained(name)\n",
        "model = AutoModelForCausalLM.from_pretrained(name)\n",
        "\n",
        "\n",
        "config = LoraConfig(r=4, lora_alpha=8, task_type=TaskType.CAUSAL_LM)\n",
        "model = get_peft_model(model, config)\n",
        "\n",
        "\n",
        "text = \"AI is changing the world.\"\n",
        "inputs = tok(text, return_tensors=\"pt\")\n",
        "loss = model(**inputs, labels=inputs[\"input_ids\"]).loss\n",
        "\n",
        "print(\"Loss:\", loss.item())\n",
        "print(\"Perplexity:\", math.exp(loss.item()))\n",
        "\n",
        "gen = pipeline(\"text-generation\", model=model, tokenizer=tok)\n",
        "print(gen(\"AI will\", max_length=40, do_sample=True)[0][\"generated_text\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWIsVAYZRgk9"
      },
      "source": [
        "#Practical no 8\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XdahCqqOQRyL"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install transformers datasets evaluate torch rouge_score -q\n",
        "\n",
        "from transformers import pipeline\n",
        "import evaluate\n",
        "\n",
        "\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", device=-1)\n",
        "\n",
        "\n",
        "text = \"\"\"Artificial Intelligence (AI) is transforming industries by automating tasks,\n",
        "improving efficiency, and enabling new innovations such as self-driving cars and smart assistants.\"\"\"\n",
        "\n",
        "\n",
        "summary = summarizer(text, max_length=40, min_length=10, do_sample=False)[0]['summary_text']\n",
        "print(\"Generated Summary:\\n\", summary)\n",
        "\n",
        "\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "results = rouge.compute(predictions=[summary], references=[\"AI is changing industries through automation and innovation.\"])\n",
        "print(\"\\nROUGE Scores:\", results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnMyHVPARjud"
      },
      "source": [
        "#practical no 9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQcKFFdZQR0m"
      },
      "outputs": [],
      "source": [
        "!pip install faiss-cpu sentence-transformers transformers torch -q\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import pipeline\n",
        "import faiss, numpy as np\n",
        "\n",
        "docs = [\n",
        "    \"Taj Mahal is in Agra, India.\",\n",
        "    \"Eiffel Tower is in Paris, France.\",\n",
        "    \"Great Wall of China is very long.\",\n",
        "    \"Mount Everest is the tallest mountain.\"\n",
        "]\n",
        "\n",
        "embed = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "vecs = embed.encode(docs)\n",
        "\n",
        "index = faiss.IndexFlatL2(vecs.shape[1])\n",
        "index.add(vecs)\n",
        "\n",
        "q = \"Where is the Eiffel Tower?\"\n",
        "q_vec = embed.encode([q])\n",
        "_, idx = index.search(q_vec, 1)\n",
        "ctx = docs[idx[0][0]]\n",
        "\n",
        "gen = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
        "print(gen(f\"Context: {ctx}\\nQ: {q}\\nA:\", max_length=40)[0][\"generated_text\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kVPOMkGRkIm"
      },
      "source": [
        "Practical no 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iVYYnMvKQR3S"
      },
      "outputs": [],
      "source": [
        "!pip install transformers torch psutil -q\n",
        "\n",
        "import torch, time, psutil\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "name = \"distilbert-base-uncased\"\n",
        "tok = AutoTokenizer.from_pretrained(name)\n",
        "m32 = AutoModelForSequenceClassification.from_pretrained(name)\n",
        "\n",
        "txt = \"AI is transforming the world.\"\n",
        "x = tok(txt, return_tensors=\"pt\")\n",
        "\n",
        "t = time.time();\n",
        "with torch.no_grad(): m32(**x)\n",
        "print(\"FP32:\", round(time.time() - t, 4), \"s\")\n",
        "print(\"Mem before:\", psutil.virtual_memory().percent, \"%\")\n",
        "\n",
        "m8 = torch.quantization.quantize_dynamic(m32, {torch.nn.Linear}, dtype=torch.qint8)\n",
        "\n",
        "t = time.time();\n",
        "with torch.no_grad(): m8(**x)\n",
        "print(\"INT8:\", round(time.time() - t, 4), \"s\")\n",
        "print(\"Mem after:\", psutil.virtual_memory().percent, \"%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sc3qieX8Rk11"
      },
      "source": [
        "practical no 11\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ccwaEUd3QR5T"
      },
      "outputs": [],
      "source": [
        "!pip install transformers torch -q\n",
        "from transformers import pipeline\n",
        "\n",
        "gen = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "p = \"A nurse is usually\"\n",
        "out = gen(p, max_length=20)[0][\"generated_text\"]\n",
        "\n",
        "bad = [\"he\",\"she\",\"his\",\"her\"]\n",
        "clean = \" \".join(\"[filtered]\" if w.lower() in bad else w for w in out.split())\n",
        "\n",
        "print(\"Original:\\n\", out)\n",
        "print(\"\\nMitigated:\\n\", clean)\n",
        "print(\"\\nBias before / after:\",sum(w.lower() in bad for w in out.split()),\"/\", sum(w.lower() in bad for w in clean.split()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-_LAprNRovm"
      },
      "source": [
        "Practical no 12\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RuRJQiqlRoCG"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, name): self.name, self.energy = name, 100\n",
        "    def choose(self): return random.choice([\"coop\",\"comp\"])\n",
        "    def act(self, other):\n",
        "        a, b = self.choose(), other.choose()\n",
        "        if a==\"coop\" and b==\"coop\": self.energy+=10; other.energy+=10; r=\"ü§ù both cooperated\"\n",
        "        elif a==\"comp\" and b==\"coop\": self.energy+=20; other.energy-=10; r=f\"{self.name} exploited {other.name} üòà\"\n",
        "        elif a==\"coop\" and b==\"comp\": self.energy-=10; other.energy+=20; r=f\"{other.name} exploited {self.name} üò¢\"\n",
        "        else: self.energy-=5; other.energy-=5; r=\"‚öîÔ∏è both competed\"\n",
        "        return r\n",
        "\n",
        "A, B = Agent(\"A\"), Agent(\"B\")\n",
        "\n",
        "for i in range(5):\n",
        "    print(f\"\\nRound {i+1}\")\n",
        "    print(A.act(B))\n",
        "    print(f\"A:{A.energy} | B:{B.energy}\")\n",
        "\n",
        "print(\"\\nWinner:\", \"A‚úÖ\" if A.energy>B.energy else \"B‚úÖ\" if B.energy>A.energy else \"Tie ü§ù\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXB5uF0ERp3-"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0AbZRo-iRoFl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vR19Y9BtRoJW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QauvXE0SRoNZ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
